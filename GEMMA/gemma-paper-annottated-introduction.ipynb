{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0d589e",
   "metadata": {
    "papermill": {
     "duration": 0.020393,
     "end_time": "2024-04-26T09:34:26.503626",
     "exception": false,
     "start_time": "2024-04-26T09:34:26.483233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preliminaris\n",
    "\n",
    "I'm **experimenting** with this notebook format. While reading a paper relevant to a competition, I try to annotate it to make the content more accessible. It's a **work in progress** that I will continue to update and put effort into, should there be any interest\n",
    "\n",
    "I'm also reading and annotating the DeepSeekMath paper you can look it:\n",
    "\n",
    "https://www.kaggle.com/code/cescofran/deepseekmath-paper-annotated-introduction\n",
    "\n",
    "### Critical Reading\n",
    "\n",
    "Rember you should have a strong critical reading on this material, since\n",
    "\n",
    "1. There are some parts I'm far from beeing an expert.\n",
    "2. I try to enhance my understanding as I read, so I can annotate my state of knowledge about a line at that moment and then revise and correct it later.\n",
    "\n",
    "For those reading their first academic papers, remember that critical reading is essential even for the papers themself, especially since research in this field (often lead by privite company) can often have significant financial consequences.\n",
    "\n",
    "### Others\n",
    "\n",
    "When is possible I givea title to the paragraph that try to summarize is content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d246b",
   "metadata": {
    "papermill": {
     "duration": 0.020792,
     "end_time": "2024-04-26T09:34:26.546128",
     "exception": false,
     "start_time": "2024-04-26T09:34:26.525336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Gemma: Open Models Based on Gemini Research and Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e11b5a5",
   "metadata": {
    "papermill": {
     "duration": 0.019958,
     "end_time": "2024-04-26T09:34:26.587208",
     "exception": false,
     "start_time": "2024-04-26T09:34:26.567250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea043374",
   "metadata": {
    "papermill": {
     "duration": 0.020624,
     "end_time": "2024-04-26T09:34:26.629296",
     "exception": false,
     "start_time": "2024-04-26T09:34:26.608672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Paragraph 1 = Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d02ff",
   "metadata": {
    "papermill": {
     "duration": 0.021409,
     "end_time": "2024-04-26T09:34:26.671580",
     "exception": false,
     "start_time": "2024-04-26T09:34:26.650171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce202f",
   "metadata": {
    "papermill": {
     "duration": 0.019099,
     "end_time": "2024-04-26T09:34:26.710858",
     "exception": false,
     "start_time": "2024-04-26T09:34:26.691759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We present Gemma, a family of **open** **models** based on Google’s **Gemini** models (Gemini Team, 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d2c94",
   "metadata": {
    "papermill": {
     "duration": 0.019598,
     "end_time": "2024-04-26T09:34:26.751930",
     "exception": false,
     "start_time": "2024-04-26T09:34:26.732332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c3e7e",
   "metadata": {
    "papermill": {
     "duration": 0.020244,
     "end_time": "2024-04-26T09:34:26.791943",
     "exception": false,
     "start_time": "2024-04-26T09:34:26.771699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, seam discuss the concept of a model in the context of Large Language Models (LLMs) as described on Wikipedia.\n",
    "\n",
    "> A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f5fe6",
   "metadata": {
    "papermill": {
     "duration": 0.020176,
     "end_time": "2024-04-26T09:34:26.831833",
     "exception": false,
     "start_time": "2024-04-26T09:34:26.811657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Open"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8520b7",
   "metadata": {
    "papermill": {
     "duration": 0.018998,
     "end_time": "2024-04-26T09:34:26.871446",
     "exception": false,
     "start_time": "2024-04-26T09:34:26.852448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "What constitutes a model as open can sometimes be unclear and not straightforward. The ambiguity arises from determining how much of the model's elements need to be publicly available for it to be considered open. Some may argue that all elements should be made available, or the degreee of accessibility of related documentation play a role in the definition, and some argue that merely accessing the constituent elements of the model that allow it to run is sufficient for to be considered open."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3d6d2",
   "metadata": {
    "papermill": {
     "duration": 0.019461,
     "end_time": "2024-04-26T09:34:26.910810",
     "exception": false,
     "start_time": "2024-04-26T09:34:26.891349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Gemini models\n",
    "\n",
    "Are a family of llm models \n",
    "\n",
    "> Gemini is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, Gemini Pro, and Gemini Nano, it was announced on December 6, 2023, positioned as a competitor to OpenAI's GPT-4. It powers the chatbot of the same name. [Wikipedia]\n",
    "\n",
    "In the official page is claimed that \n",
    "\n",
    "\n",
    "> The Gemini ecosystem represents Google's most capable AI.\n",
    "\n",
    "> Our Gemini models are built from the ground up for multimodality — reasoning seamlessly across text, images, audio, video, and code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b5fd8",
   "metadata": {
    "papermill": {
     "duration": 0.020608,
     "end_time": "2024-04-26T09:34:26.951886",
     "exception": false,
     "start_time": "2024-04-26T09:34:26.931278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Paragraph 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559fb88",
   "metadata": {
    "papermill": {
     "duration": 0.019455,
     "end_time": "2024-04-26T09:34:26.991848",
     "exception": false,
     "start_time": "2024-04-26T09:34:26.972393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbfcbf",
   "metadata": {
    "papermill": {
     "duration": 0.019325,
     "end_time": "2024-04-26T09:34:27.031316",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.011991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We trained Gemma models on up to **6T tokens** of text, using **similar** *architectures*, *data*, and *training recipes* as the Gemini model family.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f641fc2d",
   "metadata": {
    "papermill": {
     "duration": 0.021247,
     "end_time": "2024-04-26T09:34:27.072746",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.051499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6T tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f494e",
   "metadata": {
    "papermill": {
     "duration": 0.019368,
     "end_time": "2024-04-26T09:34:27.113121",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.093753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "T stand for tera which is equivalent to $10^{12}$\n",
    "\n",
    "The data on which the model has been training is composed of text, that when tokenized will consist of a number of approximatly 6.000.000.000.000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798413ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T09:34:27.157020Z",
     "iopub.status.busy": "2024-04-26T09:34:27.156077Z",
     "iopub.status.idle": "2024-04-26T09:34:27.172081Z",
     "shell.execute_reply": "2024-04-26T09:34:27.170560Z"
    },
    "papermill": {
     "duration": 0.041127,
     "end_time": "2024-04-26T09:34:27.175070",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.133943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sky', 'is', 'blue']\n"
     ]
    }
   ],
   "source": [
    "text = \"The sky is blue\"\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "tokens = tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ef276b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T09:34:27.217406Z",
     "iopub.status.busy": "2024-04-26T09:34:27.216353Z",
     "iopub.status.idle": "2024-04-26T09:34:27.224833Z",
     "shell.execute_reply": "2024-04-26T09:34:27.223387Z"
    },
    "papermill": {
     "duration": 0.032752,
     "end_time": "2024-04-26T09:34:27.227728",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.194976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6480000000.0\n"
     ]
    }
   ],
   "source": [
    "char_in_a_token = 4\n",
    "chars_in_a_line = 90\n",
    "number_tokens_in_a_line_of_text = chars_in_a_line / (char_in_a_token +1 ) # +1 is the space\n",
    "number_of_lines_in_a_page = 90\n",
    "number_of_pages_in_the_books = 400\n",
    "\n",
    "number_of_books = 10_000\n",
    "\n",
    "number_of_tokens = number_tokens_in_a_line_of_text*number_of_lines_in_a_page*number_of_pages_in_the_books*number_of_books\n",
    "print(number_of_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36475b5",
   "metadata": {
    "papermill": {
     "duration": 0.022314,
     "end_time": "2024-04-26T09:34:27.271026",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.248712",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5f35e0b",
   "metadata": {
    "papermill": {
     "duration": 0.019326,
     "end_time": "2024-04-26T09:34:27.311109",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.291783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "What are tokens?\n",
    "> \n",
    "> Tokens can be thought of as pieces of words. Before the API processes the request, the input is broken down into tokens. These tokens are not cut up exactly where the words start or end - tokens can include trailing spaces and even sub-words. Here are some helpful rules of thumb for understanding tokens in terms of lengths:\n",
    "> \n",
    ">     1 token ~= 4 chars in English\n",
    "> \n",
    ">     1 token ~= ¾ words\n",
    "> \n",
    ">     100 tokens ~= 75 words\n",
    "> \n",
    "> Or \n",
    "> \n",
    ">     1-2 sentence ~= 30 tokens\n",
    "> \n",
    ">     1 paragraph ~= 100 tokens\n",
    "> \n",
    ">     1,500 words ~= 2048 tokens\n",
    "\n",
    "[source](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5318b",
   "metadata": {
    "papermill": {
     "duration": 0.019361,
     "end_time": "2024-04-26T09:34:27.351317",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.331956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3170630",
   "metadata": {
    "papermill": {
     "duration": 0.111223,
     "end_time": "2024-04-26T09:34:27.483111",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.371888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are three elements that constitute an LLM model:\n",
    "\n",
    "    - The deep learning architecture of the model, which in this case is probably a transformer-based architecture.\n",
    "    - The data on which it is trained. As stated above, the size can vary, but what is implied here is that the type of data used is the same as that used by the Gemini family.\n",
    "    - The training process it uses. There are many ways we can train a model, using different approaches, for example, to update the gradient.\n",
    "\n",
    "Gemma seems to be similar in all these three aspects to the Gemini family.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6112c",
   "metadata": {
    "papermill": {
     "duration": 0.019488,
     "end_time": "2024-04-26T09:34:27.523248",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.503760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ac041",
   "metadata": {
    "papermill": {
     "duration": 0.019649,
     "end_time": "2024-04-26T09:34:27.563641",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.543992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Like Gemini, these models achieve strong **generalist** capabilities in text domains, alongside state-of-the-art **understanding** and **reasoning** skills at scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d3e75",
   "metadata": {
    "papermill": {
     "duration": 0.020542,
     "end_time": "2024-04-26T09:34:27.605554",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.585012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### generalist \n",
    "\n",
    "It may mean that could achive strong result on different kind of text domain tasks (that called natural-language processing taks).simultaniusly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae856fe",
   "metadata": {
    "papermill": {
     "duration": 0.021869,
     "end_time": "2024-04-26T09:34:27.647342",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.625473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### understanding \n",
    "\n",
    "> Natural-language understanding (NLU) or natural-language interpretation (NLI)[1] is a subset of natural-language processing in artificial intelligence that deals with machine reading comprehension. (wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009cf17a",
   "metadata": {
    "papermill": {
     "duration": 0.019506,
     "end_time": "2024-04-26T09:34:27.687346",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.667840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### reasoning\n",
    "\n",
    "You can see a competition like this [one](https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize/overview) \n",
    "to see a kind of reasoning task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080207c6",
   "metadata": {
    "papermill": {
     "duration": 0.019697,
     "end_time": "2024-04-26T09:34:27.726939",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.707242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79768b84",
   "metadata": {
    "papermill": {
     "duration": 0.019936,
     "end_time": "2024-04-26T09:34:27.767271",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.747335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With this work, we release both **pre-trained** and **fine-tuned** **checkpoints**, as well as an open-source **codebase** for **inference** and **serving**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fa00d",
   "metadata": {
    "papermill": {
     "duration": 0.020944,
     "end_time": "2024-04-26T09:34:27.808678",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.787734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### checkpoints\n",
    "\n",
    "When training happen you can save the state of training model in checkpoints.\n",
    "You can find the checkpoints here https://www.kaggle.com/models/google/gemma/pyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167bfbf2",
   "metadata": {
    "papermill": {
     "duration": 0.020656,
     "end_time": "2024-04-26T09:34:27.849443",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.828787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### pre-trained \n",
    "\n",
    "Is the state of the model after training, is called \"pre\" because usally you need or want to do another training before making use of it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f273f",
   "metadata": {
    "papermill": {
     "duration": 0.020236,
     "end_time": "2024-04-26T09:34:27.891000",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.870764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### fine-tuned\n",
    "\n",
    "The fine tune training is the one that come after a pre-train because you have a more specific task or domain on which you want achive better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b22002",
   "metadata": {
    "papermill": {
     "duration": 0.021324,
     "end_time": "2024-04-26T09:34:27.932906",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.911582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### codebase\n",
    "\n",
    "They realise the open source codebase on github at https://github.com/google/gemma_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149213bd",
   "metadata": {
    "papermill": {
     "duration": 0.020574,
     "end_time": "2024-04-26T09:34:27.975377",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.954803",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17749d3d",
   "metadata": {
    "papermill": {
     "duration": 0.022348,
     "end_time": "2024-04-26T09:34:28.017791",
     "exception": false,
     "start_time": "2024-04-26T09:34:27.995443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### inference\n",
    "\n",
    "They provide the source code on which you want to make inference.\n",
    "\n",
    "If the model is \n",
    "\n",
    "```python\n",
    "\n",
    "class Model:\n",
    "    ...\n",
    "```\n",
    "\n",
    "You could imagine this as the code that go into:\n",
    "\n",
    "```python\n",
    "class Model:\n",
    "\n",
    "    def predict(self, sample):\n",
    "        ...\n",
    "        \n",
    "````\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1e352",
   "metadata": {
    "papermill": {
     "duration": 0.021842,
     "end_time": "2024-04-26T09:34:28.061971",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.040129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### serving\n",
    "\n",
    "This should be more genearly the code that help for loading, preprocessing and other function that could be need to use the model at inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ffbdf",
   "metadata": {
    "papermill": {
     "duration": 0.020682,
     "end_time": "2024-04-26T09:34:28.104183",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.083501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### NOTE\n",
    "\n",
    "They do not seam to deliver training code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff4ee9",
   "metadata": {
    "papermill": {
     "duration": 0.019573,
     "end_time": "2024-04-26T09:34:28.143746",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.124173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Paragraph 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2dd89",
   "metadata": {
    "papermill": {
     "duration": 0.01939,
     "end_time": "2024-04-26T09:34:28.183219",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.163829",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad1362b",
   "metadata": {
    "papermill": {
     "duration": 0.019382,
     "end_time": "2024-04-26T09:34:28.222517",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.203135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Gemma comes in two sizes: a **7 billion** parameter model for efficient deployment and development on *GPU* and *TPU*, and a **2 billion** parameter model for CPU and on-device applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be9edeb",
   "metadata": {
    "papermill": {
     "duration": 0.021646,
     "end_time": "2024-04-26T09:34:28.264203",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.242557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7 billion \n",
    "\n",
    "This seam the to be the bigger size that seam to be needed GPU and TPU to efficently been used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253dd93",
   "metadata": {
    "papermill": {
     "duration": 0.01959,
     "end_time": "2024-04-26T09:34:28.304952",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.285362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2 billion \n",
    "\n",
    "The smaller size is seam to been realized for the case where the hardware platform cannot stand the load of the 7 billion version.\n",
    "\n",
    "This should be the version you load when you do \n",
    "\n",
    "```python \n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
    "```\n",
    "\n",
    "Where `keras_nlp.models.GemmaCausalLM` is the class of your gemma model. The one that that we defined above as\n",
    "\n",
    "\n",
    "```python\n",
    "class Model:\n",
    "\n",
    "    def predict(self, sample):\n",
    "        ...\n",
    "        \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5e4b3",
   "metadata": {
    "papermill": {
     "duration": 0.019745,
     "end_time": "2024-04-26T09:34:28.344909",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.325164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### NOTE \n",
    "\n",
    "The parameters are another compoments that has been realised. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb6fbb",
   "metadata": {
    "papermill": {
     "duration": 0.019447,
     "end_time": "2024-04-26T09:34:28.384303",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.364856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276e503",
   "metadata": {
    "papermill": {
     "duration": 0.021446,
     "end_time": "2024-04-26T09:34:28.425802",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.404356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Each size is designed to address different **computational constraints**, **applications, and developer requirements**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c9d36",
   "metadata": {
    "papermill": {
     "duration": 0.019926,
     "end_time": "2024-04-26T09:34:28.466182",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.446256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### computational constraints\n",
    "\n",
    "This are the one adressed above, that is depending for example on the aviability of GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aecfb1",
   "metadata": {
    "papermill": {
     "duration": 0.020796,
     "end_time": "2024-04-26T09:34:28.508039",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.487243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### applications\n",
    "\n",
    "There may be application where you may pick differnt trade off beetween accuracy and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e065954",
   "metadata": {
    "papermill": {
     "duration": 0.020355,
     "end_time": "2024-04-26T09:34:28.549254",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.528899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### developer requirements\n",
    "\n",
    "For example a developer can need a model that more light and easy to iterate, since iteration are fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31008a5",
   "metadata": {
    "papermill": {
     "duration": 0.019486,
     "end_time": "2024-04-26T09:34:28.588782",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.569296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935cab5",
   "metadata": {
    "papermill": {
     "duration": 0.019454,
     "end_time": "2024-04-26T09:34:28.628261",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.608807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "At each scale, we release raw, pre-trained checkpoints, as well as checkpoints fine-tuned for **dialogue**, **instruction-following**, **help-fulness**, and **safety**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2d0d0",
   "metadata": {
    "papermill": {
     "duration": 0.020238,
     "end_time": "2024-04-26T09:34:28.668286",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.648048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### dialogue \n",
    "\n",
    "I think this is so that the model give higher probability to world that go in the direction of seam a diagloque."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307d1180",
   "metadata": {
    "papermill": {
     "duration": 0.020432,
     "end_time": "2024-04-26T09:34:28.708848",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.688416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### instruction-following\n",
    "\n",
    "I think here they orient the model at be more carefull to follow the instruction rathern then take more liberty to intepretet the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169dfe1",
   "metadata": {
    "papermill": {
     "duration": 0.021206,
     "end_time": "2024-04-26T09:34:28.752654",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.731448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### help-fulness\n",
    "\n",
    "I think here they optimize on the prediction be actual on some help, rather then on other criteria like for example creativity,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce7f36b",
   "metadata": {
    "papermill": {
     "duration": 0.020227,
     "end_time": "2024-04-26T09:34:28.793396",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.773169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### safety\n",
    "\n",
    "The model by iteself could produce text that can harmfull to some audience, so I think here they try to discorage model for learning pattern that can lead to this harmufll content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386469c8",
   "metadata": {
    "papermill": {
     "duration": 0.020012,
     "end_time": "2024-04-26T09:34:28.833176",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.813164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### NOTE \n",
    "\n",
    "There seam not be 4 releases  (dialogue, intruction, etc..) which each one optimize on a component, but for each fine-tuned model they try at the same time to optimzize on this 4 dimensions.\n",
    "\n",
    "In the checkpoints state of pytorch implementation they refer to this fine tuned model as IT for [I]nstruction [T]uned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d645e1",
   "metadata": {
    "papermill": {
     "duration": 0.019528,
     "end_time": "2024-04-26T09:34:28.872788",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.853260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20565296",
   "metadata": {
    "papermill": {
     "duration": 0.019485,
     "end_time": "2024-04-26T09:34:28.912032",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.892547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We thoroughly evaluate the **shortcomings** of our models on a suite of **quantitative** and **qualitative** benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369d3e8",
   "metadata": {
    "papermill": {
     "duration": 0.021356,
     "end_time": "2024-04-26T09:34:28.953757",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.932401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf2f59",
   "metadata": {
    "papermill": {
     "duration": 0.021532,
     "end_time": "2024-04-26T09:34:28.997038",
     "exception": false,
     "start_time": "2024-04-26T09:34:28.975506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We believe the release of both pretrained and fine-tuned check-points will enable thorough research and investigation into the impact of current **instruction-tuning regimes**, as well as the development of increasingly **safe and responsible** model development methodologies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b025189",
   "metadata": {
    "papermill": {
     "duration": 0.022407,
     "end_time": "2024-04-26T09:34:29.041906",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.019499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### NOTE\n",
    "\n",
    "Here seam to imply some motivation around delivered the checkpoings\n",
    "\n",
    "1. Improve research on Instruction oriented model\n",
    "2. Make the development of LLM more safe and responsible\n",
    "\n",
    "The second could want suggest that people that develop model have more open access to the fundational model, \n",
    "can develop more saftly and responsably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd31002f",
   "metadata": {
    "papermill": {
     "duration": 0.020818,
     "end_time": "2024-04-26T09:34:29.082899",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.062081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Paragraph 4 = Advance Claim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e24f1",
   "metadata": {
    "papermill": {
     "duration": 0.019647,
     "end_time": "2024-04-26T09:34:29.124463",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.104816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Line 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc875d",
   "metadata": {
    "papermill": {
     "duration": 0.019706,
     "end_time": "2024-04-26T09:34:29.165390",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.145684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Gemma **advances** state-of-the-art performance relative to **comparable-scale** (and some larger), **open** models (Almazrouei et al., 2023; Jiang et al., 2023; Touvron et al., 2023a,b) across wide range of domains including both automated benchmarks and human evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576767a",
   "metadata": {
    "papermill": {
     "duration": 0.020756,
     "end_time": "2024-04-26T09:34:29.206228",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.185472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Open\n",
    "\n",
    "Here refer to some open model like\n",
    "\n",
    "[1] The falcon series of open language models,==> Almazrouei et al., 2023.\n",
    "\n",
    "[2] Mistral ===> 7b Jiang et al., 2023\n",
    "\n",
    "[3] Llama: Open and efficient foundation language models ===> Touvron et al., 2023a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6af13",
   "metadata": {
    "papermill": {
     "duration": 0.020411,
     "end_time": "2024-04-26T09:34:29.246845",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.226434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Line 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d397a",
   "metadata": {
    "papermill": {
     "duration": 0.020542,
     "end_time": "2024-04-26T09:34:29.288518",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.267976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Example **domains** include **question answering** (Clark et al., 2019; Kwiatkowski et al., 2019), **commonsense reasoning** (Sakaguchi et al., 2019; Suzgun et al.,2022), **mathematics and science** (Cobbe et al.,\n",
    "2021; Hendrycks et al., 2020), and **coding** (Austin et al., 2021; Chen et al., 2021). See complete details in the Evaluation section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677729ec",
   "metadata": {
    "papermill": {
     "duration": 0.020166,
     "end_time": "2024-04-26T09:34:29.330140",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.309974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Question answering\n",
    "\n",
    "Can the llm prediction be used for answer natural language question?\n",
    "In (Clark et al., 2019) they try to consider the special case of Yes/No questions.\n",
    " \n",
    "#### Commonsense reasoning\n",
    "\n",
    "For example in the case of  (Sakaguchi et al., 2019) the abstract say \"is a set of 273 expert-crafted pronoun resolution problems\".\n",
    "\n",
    "#### Mathematics and science\n",
    "\n",
    "Can the llm prediction be used to solve math or more generally scienitfic question?\n",
    "\n",
    "\n",
    "#### Coding\n",
    "\n",
    "Can the llm prediction be used for generate code?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d804932",
   "metadata": {
    "papermill": {
     "duration": 0.020283,
     "end_time": "2024-04-26T09:34:29.370526",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.350243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Paragraph 5 = The Origin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a0d29",
   "metadata": {
    "papermill": {
     "duration": 0.02279,
     "end_time": "2024-04-26T09:34:29.415522",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.392732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 1\n",
    "\n",
    "Like Gemini, Gemma builds on recent work on **sequence models** (Sutskever et al., 2014) and **transformers** (Vaswani et al., 2017), **deep learning** methods based on neural networks (LeCun et al., 2015), and techniques for **large-scale training** on distributed systems (Barham et al., 2022; Dean et al., 2012; Roberts et al., 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7867bdc",
   "metadata": {
    "papermill": {
     "duration": 0.019515,
     "end_time": "2024-04-26T09:34:29.459328",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.439813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### sequence models\n",
    "\n",
    "Sequence to Sequence Learning with Neural Networks  (Sutskever et al., 2014)\n",
    "\n",
    "from the abstract \n",
    ">  Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. \n",
    "\n",
    "\n",
    "#### transformers\n",
    "\n",
    "This is the deep learning architecture that has directly inspired many LLM deep learning architectures. Note that the architecture has two parts: an encoder and a decoder model, which I think were considered more of an evolution of the sequence-to-sequence model often used for translation task, where you may want a part that encode what you want translate an a part that decode the translation.\n",
    "\n",
    "In LLM model the part that has been more extensivly used is the decoder part. If you think the encoder somhow can be seen as an understanding part, and the decoder as a generting part, if you think this way is natural to expect that is just this part that will be the focus of people that want build generative models.\n",
    "\n",
    "#### Deep Learing\n",
    "\n",
    "Here the authors quote not the article that have set up the birth of deep learning (a single article by the way would be difficult to point out) but an article that as been publish in a very popular scientific journal like nature\n",
    "https://www.nature.com/articles/nature14539\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d229f7f",
   "metadata": {
    "papermill": {
     "duration": 0.019975,
     "end_time": "2024-04-26T09:34:29.499191",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.479216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 2\n",
    "\n",
    "Gemma also builds on Google’s long **history of open models** and ecosystems, including **Word2Vec** (Mikolov et al., 2013), the **Transformer** (Vaswani et al.,2017), **BERT** (Devlin et al., 2018), and **T5** (Raffel et al., 2019) and **T5X** (Roberts et al., 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a95ed4",
   "metadata": {
    "papermill": {
     "duration": 0.026129,
     "end_time": "2024-04-26T09:34:29.547508",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.521379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Word2Vec\n",
    "\n",
    "This model learns just by observing how two words co-occur. It is a very popular algorithm used to find word representations, which, unlike those learned by transformers, are independent of context. Therefore, to give a representation to a word, you do not need to know its context.\n",
    "\n",
    "Bert\n",
    "\n",
    "We talk above of encoder and decoder, Bert is a model that develop the encoder part of the tranformer architeture.\n",
    "\n",
    "T5 and TSX\n",
    "\n",
    "More like the origninal tranfomer presentation this model have the 2 parts encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62000ffd",
   "metadata": {
    "papermill": {
     "duration": 0.028175,
     "end_time": "2024-04-26T09:34:29.600377",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.572202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Paragraph 6 = Responsible LLM Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a995e",
   "metadata": {
    "papermill": {
     "duration": 0.02886,
     "end_time": "2024-04-26T09:34:29.658241",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.629381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885eeacb",
   "metadata": {
    "papermill": {
     "duration": 0.022668,
     "end_time": "2024-04-26T09:34:29.706904",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.684236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We believe the **responsible release of LLMs** is critical for improving the **safety** of **frontier models**, for ensuring **equitable access** to this breakthrough technology, for enabling **rigorous evaluation and analysis** of current techniques, and for enabling the development of the **next wave** of innovations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b89f3c",
   "metadata": {
    "papermill": {
     "duration": 0.020845,
     "end_time": "2024-04-26T09:34:29.748269",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.727424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40bdcde",
   "metadata": {
    "papermill": {
     "duration": 0.019721,
     "end_time": "2024-04-26T09:34:29.788060",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.768339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "While **thorough testing** of all Gemma models has been conducted, **testing cannot cover** all applications and scenarios in which Gemma may be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9370a0f",
   "metadata": {
    "papermill": {
     "duration": 0.024612,
     "end_time": "2024-04-26T09:34:29.835021",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.810409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## thorough testing \n",
    "\n",
    "It would be intersting here undestand more quantitavly what \"thorough\" mean\n",
    "\n",
    "## testing cannot cover\n",
    "\n",
    "Here the author warn the user of developer of an LLM based on this foundational model of the difficulty \n",
    "of warranty safty for applications that they could not have anticiapted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b4e14b",
   "metadata": {
    "papermill": {
     "duration": 0.020803,
     "end_time": "2024-04-26T09:34:29.880531",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.859728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a3dd92",
   "metadata": {
    "papermill": {
     "duration": 0.019979,
     "end_time": "2024-04-26T09:34:29.923286",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.903307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With this in mind, **all Gemma users should conduct rigorous safety testing** specific to their use case before deployment or use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13432c0f",
   "metadata": {
    "papermill": {
     "duration": 0.020352,
     "end_time": "2024-04-26T09:34:29.966363",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.946011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the dedicated section they claim have released \n",
    "> We have also released a Generative AI Responsible Toolkit to support developers to build AI responsibly. \n",
    "\n",
    "Which should be descibed here:\n",
    "https://ai.google.dev/responsible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82a0358",
   "metadata": {
    "papermill": {
     "duration": 0.02114,
     "end_time": "2024-04-26T09:34:30.009704",
     "exception": false,
     "start_time": "2024-04-26T09:34:29.988564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d864d",
   "metadata": {
    "papermill": {
     "duration": 0.024191,
     "end_time": "2024-04-26T09:34:30.057366",
     "exception": false,
     "start_time": "2024-04-26T09:34:30.033175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**More details** on our approach to safety can be found in section Responsible Deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883412eb",
   "metadata": {
    "papermill": {
     "duration": 0.021796,
     "end_time": "2024-04-26T09:34:30.100676",
     "exception": false,
     "start_time": "2024-04-26T09:34:30.078880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Paragraph 7 = Paper Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b14d233",
   "metadata": {
    "papermill": {
     "duration": 0.022092,
     "end_time": "2024-04-26T09:34:30.143958",
     "exception": false,
     "start_time": "2024-04-26T09:34:30.121866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Line 1\n",
    "\n",
    "In this **technical report**, we provide a detailed overview of the **model architecture**, **training infrastructure**, and **pretraining and fine-tuning recipes** for Gemma, followed by thorough **evaluations of all checkpoints** across a wide-variety of quantitative and qualitative benchmarks, as well as both **standard academic benchmarks** and **human-preference evaluations**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b48e1d",
   "metadata": {
    "papermill": {
     "duration": 0.01988,
     "end_time": "2024-04-26T09:34:30.184733",
     "exception": false,
     "start_time": "2024-04-26T09:34:30.164853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Technical report \n",
    "\n",
    "Is a form of technical writing \n",
    "I try to summarize wikipeda view on it with 3 quotes\n",
    "\n",
    "> A technical report (also scientific report) is a document that describes the process, progress, or results of technical or scientific research or the state of a technical or scientific research problem.\n",
    "\n",
    "> Researchers may also publish work in early form as a technical report to establish novelty, without having to wait for the often long production schedules of academic journals.\n",
    "\n",
    "> Unlike other scientific literature, such as scientific journals and the proceedings of some academic conferences, technical reports rarely undergo comprehensive independent peer review before publication. \n",
    "\n",
    "\n",
    "#### Model architeture\n",
    "\n",
    "In the Model Architeture section they claim \n",
    "\n",
    ">The Gemma model architecture is based on the transformer decoder (Vaswani et al., 2017).\n",
    "\n",
    "#### Training Infrastructure\n",
    "\n",
    "In the dedicated Training Infrastructure section theygo specifc on what kind of harware they use, for example \n",
    "\n",
    "> We train the Gemma models using TPUv5e; TPUv5e are deployed in pods of 256 chips, configured into a 2D torus of 16 x 16 chips. \n",
    "\n",
    "But also they have a subsection on Carbon Footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a83886",
   "metadata": {
    "papermill": {
     "duration": 0.020501,
     "end_time": "2024-04-26T09:34:30.225829",
     "exception": false,
     "start_time": "2024-04-26T09:34:30.205328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We then discuss in **detail** our approach to **safe and responsible** deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9df92",
   "metadata": {
    "papermill": {
     "duration": 0.020734,
     "end_time": "2024-04-26T09:34:30.266546",
     "exception": false,
     "start_time": "2024-04-26T09:34:30.245812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, we outline the **broader implications** of Gemma, its **limitations** and **advantages**, and conclusions."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4484051,
     "sourceId": 7711309,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 5171,
     "sourceId": 11371,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.866775,
   "end_time": "2024-04-26T09:34:30.712472",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-26T09:34:22.845697",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
